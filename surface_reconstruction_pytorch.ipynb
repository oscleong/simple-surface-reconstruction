{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "import ot # optimal transport solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data points, e.g. noisy points on a 2-d plane living in 3-d\n",
    "num_points = 40\n",
    "noise_var = 0.05\n",
    "\n",
    "def sample_plane(num_points, noise_var):\n",
    "    X = torch.empty(num_points, 3).uniform_(0, 2)\n",
    "    X[:,2] = 0\n",
    "    noise = noise_var * torch.randn(num_points, 3)\n",
    "    \n",
    "    return X.double(), noise.double()\n",
    "\n",
    "def sample_sphere(num_points, noise_var):\n",
    "    X = np.random.randn(3, num_points)\n",
    "    X /= np.linalg.norm(X, axis = 0)\n",
    "    X = X.T\n",
    "    X_torch = torch.from_numpy(X)\n",
    "    noise = noise_var * torch.randn(num_points, 3)\n",
    "    \n",
    "    return X_torch.double(), noise.double()\n",
    "\n",
    "def pairwise_distance_matrix(X, Y=None):\n",
    "    '''\n",
    "    Input: x is a Nxd matrix\n",
    "           y is an optional Mxd matirx\n",
    "    Output: dist is a NxM matrix where dist[i,j] is the square norm between x[i,:] and y[j,:]\n",
    "            if y is not given then use 'y=x'.\n",
    "    i.e. dist[i,j] = ||x[i,:]-y[j,:]||^2\n",
    "    '''\n",
    "    X_norm = (X**2).sum(1).view(-1, 1)\n",
    "    if Y is not None:\n",
    "        Y_t = torch.transpose(Y, 0, 1)\n",
    "        Y_norm = (Y**2).sum(1).view(1, -1)\n",
    "    else:\n",
    "        Y_t = torch.transpose(X, 0, 1)\n",
    "        X_norm = X_norm.view(1, -1)\n",
    "    \n",
    "    Dist = X_norm + Y_norm - 2.0 * torch.mm(X, Y_t)\n",
    "    # Ensure diagonal is zero if X=Y\n",
    "    # if Y is None:\n",
    "    #     Dist = Dist - torch.diag(Dist.diag)\n",
    "    return Dist #torch.clamp(Dist, 0.0, np.inf)\n",
    "\n",
    "X, noise = sample_plane(num_points, noise_var)\n",
    "X = X.float()\n",
    "Y = X + noise.float()\n",
    "noise_level = (noise**2).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build network\n",
    "# num_params = 2*hidden1 + hidden1*3\n",
    "from torchsummary import summary\n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "\n",
    "N = num_points\n",
    "D_in = 2\n",
    "D_out = 3\n",
    "hidden1 = 200\n",
    "hidden2 = 400\n",
    "\n",
    "# Models\n",
    "v1 = torch.randn((N, D_in), device = device, dtype = dtype, requires_grad = True)\n",
    "v2 = torch.randn((N, D_in), device = device, dtype = dtype, requires_grad = True)\n",
    "model1 = torch.nn.Sequential(\n",
    "        torch.nn.Linear(D_in, hidden1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden1, hidden2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden2, D_out))\n",
    "model2 = torch.nn.Sequential(\n",
    "        torch.nn.Linear(D_in, hidden1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden1, hidden2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden2, D_out))\n",
    "Y1_pred = model1(v1)\n",
    "Y2_pred = model2(v2)\n",
    "\n",
    "def project_stochastic_to_perm(P):\n",
    "    max_in_rows = np.amax(P, axis = 1)\n",
    "    max_list = [np.where(P == max_in_rows[ii]) for ii in range(P.shape[0])]\n",
    "    for ii in range(len(max_list)):\n",
    "        P[max_list[ii]] = 1\n",
    "    Omega = np.array((P == 1))\n",
    "    return np.multiply(Omega, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization preliminaries:\n",
    "error_train_net1 = []\n",
    "error_test_net1 = []\n",
    "error_train_net2 = []\n",
    "error_test_net2 = []\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction = \"sum\")\n",
    "iteration = 0\n",
    "maxiter = 10000\n",
    "lr = 1e-4\n",
    "lam = 0.01\n",
    "beta = 0 # if beta > 0, then we add a consistency loss when fitting local patches\n",
    "optimizer_w1 = torch.optim.Adam(model1.parameters(), lr)\n",
    "optimizer_w2 = torch.optim.Adam(model2.parameters(), lr)\n",
    "\n",
    "# Get permutation matrices using EMD or sinkhorn\n",
    "C1 = pairwise_distance_matrix(Y1_pred, Y).double()\n",
    "C2 = pairwise_distance_matrix(Y2_pred, Y).double()\n",
    "a = np.ones((num_points,))\n",
    "b = a    \n",
    "C1hat = C1.clone().detach().numpy()\n",
    "C2hat = C2.clone().detach().numpy()\n",
    "# Use EMD\n",
    "P1hat = ot.emd(a, b, C1hat, numItermax=100000, log=False) \n",
    "P2hat = ot.emd(a, b, C2hat, numItermax=100000, log=False)\n",
    "# Or Sinkhorn\n",
    "#P1hat = ot.sinkhorn(a, b, C1hat, lam, method='sinkhorn', numItermax=100000, stopThr=1e-09)\n",
    "#P2hat = ot.sinkhorn(a, b, C2hat, lam, method='sinkhorn', numItermax=100000, stopThr=1e-09)\n",
    "\n",
    "P1hat_inv = np.linalg.inv(P1hat)\n",
    "P2hat_inv = np.linalg.inv(P2hat)\n",
    "\n",
    "P1 = Variable(torch.from_numpy(P1hat), requires_grad=False)\n",
    "P2 = Variable(torch.from_numpy(P2hat), requires_grad=False)\n",
    "\n",
    "# Get permutation pi_p->q between parametric indices\n",
    "P3 = Variable(torch.from_numpy(np.matmul(P1hat, P2hat_inv)), requires_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting of local patches...\n",
      "Iteration:0, Training loss (net 1):93.33770030736923, Test loss (net 1):95.06112602353096\n",
      "Iteration:0, Training loss (net 2):101.18487125635147, Test loss (net 2):102.99303275346756\n",
      "Noise level:0.28329038341339585\n",
      "Iteration:500, Training loss (net 1):1.826964259147644, Test loss (net 1):2.057450234889984\n",
      "Iteration:500, Training loss (net 2):1.7186776399612427, Test loss (net 2):1.930122435092926\n",
      "Noise level:0.28329038341339585\n",
      "Iteration:1000, Training loss (net 1):1.0343542695045471, Test loss (net 1):1.3066586256027222\n",
      "Iteration:1000, Training loss (net 2):0.9485240578651428, Test loss (net 2):1.1981324553489685\n",
      "Noise level:0.28329038341339585\n",
      "Iteration:1500, Training loss (net 1):0.6668016910552979, Test loss (net 1):0.9530415534973145\n",
      "Iteration:1500, Training loss (net 2):0.5634211301803589, Test loss (net 2):0.8280289769172668\n",
      "Noise level:0.28329038341339585\n",
      "Iteration:2000, Training loss (net 1):0.4089251756668091, Test loss (net 1):0.6861580014228821\n",
      "Iteration:2000, Training loss (net 2):0.31759899854660034, Test loss (net 2):0.5915336012840271\n",
      "Noise level:0.28329038341339585\n",
      "Iteration:2500, Training loss (net 1):0.21498513221740723, Test loss (net 1):0.4840403199195862\n",
      "Iteration:2500, Training loss (net 2):0.16199404001235962, Test loss (net 2):0.44088876247406006\n",
      "Noise level:0.28329038341339585\n",
      "Iteration:3000, Training loss (net 1):0.10140383243560791, Test loss (net 1):0.3645128607749939\n",
      "Iteration:3000, Training loss (net 2):0.0619465708732605, Test loss (net 2):0.34303224086761475\n",
      "Noise level:0.28329038341339585\n",
      "Iteration:3500, Training loss (net 1):0.05462443828582764, Test loss (net 1):0.32077932357788086\n",
      "Iteration:3500, Training loss (net 2):0.021165311336517334, Test loss (net 2):0.3064616322517395\n",
      "Noise level:0.28329038341339585\n",
      "Iteration:4000, Training loss (net 1):0.03274798393249512, Test loss (net 1):0.3029124140739441\n",
      "Iteration:4000, Training loss (net 2):0.010010004043579102, Test loss (net 2):0.29870498180389404\n",
      "Noise level:0.28329038341339585\n",
      "Iteration:4500, Training loss (net 1):0.01817142963409424, Test loss (net 1):0.2896345257759094\n",
      "Iteration:4500, Training loss (net 2):0.0073049068450927734, Test loss (net 2):0.29748839139938354\n",
      "Noise level:0.28329038341339585\n",
      "Iteration:5000, Training loss (net 1):0.009296536445617676, Test loss (net 1):0.2819139361381531\n",
      "Iteration:5000, Training loss (net 2):0.005994915962219238, Test loss (net 2):0.2966446280479431\n",
      "Noise level:0.28329038341339585\n",
      "Iteration:5500, Training loss (net 1):0.00438922643661499, Test loss (net 1):0.27959704399108887\n",
      "Iteration:5500, Training loss (net 2):0.0046942830085754395, Test loss (net 2):0.29451602697372437\n",
      "Noise level:0.28329038341339585\n",
      "Iteration:6000, Training loss (net 1):0.0017549991607666016, Test loss (net 1):0.2796347737312317\n",
      "Iteration:6000, Training loss (net 2):0.003453552722930908, Test loss (net 2):0.2924093008041382\n",
      "Noise level:0.28329038341339585\n",
      "Iteration:6500, Training loss (net 1):0.0006325840950012207, Test loss (net 1):0.2796823978424072\n",
      "Iteration:6500, Training loss (net 2):0.0023857951164245605, Test loss (net 2):0.2906087040901184\n",
      "Noise level:0.28329038341339585\n",
      "Iteration:7000, Training loss (net 1):0.000306546688079834, Test loss (net 1):0.28290772438049316\n",
      "Iteration:7000, Training loss (net 2):0.0017549395561218262, Test loss (net 2):0.28892678022384644\n",
      "Noise level:0.28329038341339585\n",
      "Iteration:7500, Training loss (net 1):0.0002785921096801758, Test loss (net 1):0.2813212275505066\n",
      "Iteration:7500, Training loss (net 2):0.0010309219360351562, Test loss (net 2):0.28801584243774414\n",
      "Noise level:0.28329038341339585\n",
      "Iteration:8000, Training loss (net 1):9.655952453613281e-06, Test loss (net 1):0.2830214500427246\n",
      "Iteration:8000, Training loss (net 2):0.0007080435752868652, Test loss (net 2):0.288382351398468\n",
      "Noise level:0.28329038341339585\n",
      "Iteration:8500, Training loss (net 1):1.0311603546142578e-05, Test loss (net 1):0.2833870053291321\n",
      "Iteration:8500, Training loss (net 2):0.00042003393173217773, Test loss (net 2):0.28683435916900635\n",
      "Noise level:0.28329038341339585\n",
      "Iteration:9000, Training loss (net 1):0.00015479326248168945, Test loss (net 1):0.28305524587631226\n",
      "Iteration:9000, Training loss (net 2):0.0012846589088439941, Test loss (net 2):0.28976911306381226\n",
      "Noise level:0.28329038341339585\n",
      "Iteration:9500, Training loss (net 1):1.8477439880371094e-06, Test loss (net 1):0.2833406925201416\n",
      "Iteration:9500, Training loss (net 2):0.00155562162399292, Test loss (net 2):0.2863805294036865\n",
      "Noise level:0.28329038341339585\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 1: fit local patches to the surface\n",
    "print(\"Fitting of local patches...\")\n",
    "while iteration < maxiter:\n",
    "        \n",
    "    # Prediction & pairwise distance matrices \n",
    "    Y1_pred = model1(v1)\n",
    "    Y2_pred = model2(v2)\n",
    "    C1 = pairwise_distance_matrix(Y1_pred, Y).double()\n",
    "    C2 = pairwise_distance_matrix(Y2_pred, Y).double()\n",
    "    C1x = pairwise_distance_matrix(Y1_pred, X).double()\n",
    "    C2x = pairwise_distance_matrix(Y2_pred, X).double()\n",
    "    \n",
    "    # Losses\n",
    "    # Train loss\n",
    "    train_loss_net1 = torch.mul(P1, C1).sum().sum()\n",
    "    train_loss_net2 = torch.mul(P2, C2).sum().sum()\n",
    "    if beta > 0:\n",
    "        C3 = pairwise_distance_matrix(Y1_pred, Y2_pred).double()\n",
    "        train_consistency = torch.mul(P3, C3).sum().sum()\n",
    "        total_train_loss = train_loss_net1 + train_loss_net2 + beta*train_consistency\n",
    "    else:\n",
    "        total_train_loss = train_loss_net1 + train_loss_net2\n",
    "    \n",
    "    # Test loss\n",
    "    test_loss_net1 = torch.mul(P1, C1x).sum().sum() \n",
    "    test_loss_net2 = torch.mul(P2, C2x).sum().sum()    \n",
    "        \n",
    "    # Optimization\n",
    "    optimizer_w1.zero_grad()\n",
    "    optimizer_w2.zero_grad()\n",
    "    total_train_loss.backward()\n",
    "    optimizer_w1.step()\n",
    "    optimizer_w2.step()\n",
    "        \n",
    "    if iteration % 500 == 0:\n",
    "        print(\"Iteration:\" + str(iteration) \n",
    "                  + \", Training loss (net 1):\" + str(train_loss_net1.item()) \n",
    "                  + \", Test loss (net 1):\" + str(test_loss_net1.item()))\n",
    "        print(\"Iteration:\" + str(iteration) \n",
    "                  + \", Training loss (net 2):\" + str(train_loss_net2.item()) \n",
    "                  + \", Test loss (net 2):\" + str(test_loss_net2.item()))\n",
    "        print(\"Noise level:\" + str(noise_level.item()))\n",
    "\n",
    "    error_train_net1.append(train_loss_net1.item())\n",
    "    error_train_net2.append(train_loss_net2.item())\n",
    "    error_test_net1.append(test_loss_net1.item())\n",
    "    error_test_net2.append(test_loss_net2.item())\n",
    "    \n",
    "    iteration += 1\n",
    "print('--------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consistency fitting...\n",
      "Iteration:0, Training loss (net 1):0.0007739067077636719, Test loss (net 1):0.2833601236343384\n",
      "Iteration:0, Training loss (net 2):6.818771362304688e-05, Test loss (net 2):0.28439807891845703\n",
      "Iteration:0, Consistency loss:0.0005787014961242676\n",
      "Noise level:0.28329038341339585\n",
      "Iteration:500, Training loss (net 1):0.09056711196899414, Test loss (net 1):0.4015588164329529\n",
      "Iteration:500, Training loss (net 2):0.09056729078292847, Test loss (net 2):0.401559054851532\n",
      "Iteration:500, Consistency loss:-3.5762786865234375e-07\n",
      "Noise level:0.28329038341339585\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Ensure that the output of the networks is consistent with one another\n",
    "print(\"Consistency fitting...\")\n",
    "consistency_loss = []\n",
    "iteration = 0\n",
    "if beta > 0:\n",
    "    maxiter = 0\n",
    "else:\n",
    "    maxiter = 1000\n",
    "\n",
    "while iteration < maxiter:\n",
    "    \n",
    "    # Prediction & pairwise distance matrices\n",
    "    Y1_pred = model1(v1)\n",
    "    Y2_pred = model2(v2)\n",
    "    C1 = pairwise_distance_matrix(Y1_pred, Y).double()\n",
    "    C2 = pairwise_distance_matrix(Y2_pred, Y).double()    \n",
    "    C3 = pairwise_distance_matrix(Y1_pred, Y2_pred).double()\n",
    "    C1x = pairwise_distance_matrix(Y1_pred, X).double()\n",
    "    C2x = pairwise_distance_matrix(Y2_pred, X).double()   \n",
    "            \n",
    "    # Losses\n",
    "    train_loss_net1 = torch.mul(P1, C1).sum().sum()\n",
    "    train_loss_net2 = torch.mul(P2, C2).sum().sum()\n",
    "    train_consistency = torch.mul(P3, C3).sum().sum()\n",
    "    test_loss_net1 = torch.mul(P1, C1x).sum().sum() \n",
    "    test_loss_net2 = torch.mul(P2, C2x).sum().sum()  \n",
    "    \n",
    "    # Optimization\n",
    "    optimizer_w1.zero_grad()\n",
    "    optimizer_w2.zero_grad()\n",
    "    train_consistency.backward()\n",
    "    optimizer_w1.step()\n",
    "    optimizer_w2.step()\n",
    "    \n",
    "    if iteration % 500 == 0:\n",
    "        print(\"Iteration:\" + str(iteration) + \", Training loss (net 1):\" + str(train_loss_net1.item())\n",
    "              + \", Test loss (net 1):\" + str(test_loss_net1.item()))\n",
    "        print(\"Iteration:\" + str(iteration) + \", Training loss (net 2):\" + str(train_loss_net2.item())\n",
    "              + \", Test loss (net 2):\" + str(test_loss_net2.item()))\n",
    "        print(\"Iteration:\" + str(iteration) + \", Consistency loss:\" + str(train_consistency.item()))    \n",
    "        print(\"Noise level:\" + str(noise_level.item()))\n",
    "        \n",
    "    error_train_net1.append(train_loss_net1.item())\n",
    "    error_train_net2.append(train_loss_net2.item())    \n",
    "    error_test_net1.append(test_loss_net1.item())\n",
    "    error_test_net2.append(test_loss_net2.item())\n",
    "    \n",
    "    iteration += 1\n",
    "print('--------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684302bedb0042a58fb783492527000b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9c8c22588645a191068f80c9ba251b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52580c2812c4983b2d92d8a8d39ba14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%matplotlib widget\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "Yhat1 = Y1_pred.detach().numpy()\n",
    "Yhat2 = Y2_pred.detach().numpy()\n",
    "#yt_limit = noise_level + 1e-2\n",
    "#yb_limit = max(noise_level - 1e-2, 1e-6)\n",
    "\n",
    "# Plot all losses(training and test)\n",
    "plt.figure()\n",
    "plt.plot(error_train_net1, color='b', linestyle='-', label='Train Loss (net 1)')\n",
    "plt.plot(error_test_net1, color='b', linestyle='--', label='Test Loss (net 1)')\n",
    "plt.plot(error_train_net2, color='r', linestyle='-', label='Train Loss (net 2)')\n",
    "plt.plot(error_test_net2, color='r', linestyle='--',  label='Test Loss (net 2)')\n",
    "plt.axhline(y=noise_level, color='m', linestyle='-.', label='noise level')\n",
    "plt.ylabel('l2 loss')\n",
    "plt.xlabel('number of iterations')\n",
    "plt.yscale('log')\n",
    "plt.title('Comparison of losses for each net over time')\n",
    "plt.legend()\n",
    "\n",
    "# Plot data points\n",
    "fig, ax = plt.subplots(1, 1, subplot_kw={'projection':'3d', 'aspect':'equal'})\n",
    "ax.scatter(Y[:100,0], Y[:100,1], Y[:100,2], s=30, c='b', zorder=10, label='noisy points')\n",
    "ax.scatter(X[:100,0], X[:100,1], X[:100,2], s=30, c='r', zorder=10, label='clean points')\n",
    "ax.legend()\n",
    "\n",
    "# Plot reconstructed points\n",
    "fig, ax = plt.subplots(1, 1, subplot_kw={'projection':'3d', 'aspect':'equal'})\n",
    "ax.scatter(Yhat1[:100,0], Yhat1[:100,1], Yhat1[:100,2], s=30, c='b', zorder=10, label = 'recovered pts 1')\n",
    "ax.scatter(Yhat2[:100,0], Yhat2[:100,1], Yhat2[:100,2], s=30, c='r', zorder=10, label = 'recovered pts 2')\n",
    "ax.scatter(Y[:100,0], Y[:100,1], Y[:100,2], s = 30, c = 'g', zorder = 10, label = 'noisy pts')\n",
    "ax.scatter(X[:100,0], X[:100,1], X[:100,2], s = 30, c = 'k', zorder = 10, label = 'clean pts')\n",
    "ax.legend()\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
